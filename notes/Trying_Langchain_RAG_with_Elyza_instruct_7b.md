# Elyza 7b ã‚’ç”¨ã„ã¦ RAG ã‚’è©¦ã—ã¦ã¿ãŸã€‚

ä»Šå›ã¯ LLM ã«ã¯ Elyza 7B Instruct ã‚’ç”¨ã„ã€Langchain ã‚’ä½¿ã£ãŸ RAG (Retrieval Augmented Generation) ã‚’è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚

RAG ã‚’ç”¨ã„ã‚‹ã“ã¨ã§è³ªå•ã«å¯¾ã—ã¦é–¢é€£æ€§ã®é«˜ã„æ–‡ç« ã‚’æŠ½å‡ºã—ã€ã‚ˆã‚Šé©åˆ‡ãªç­”ãˆã‚’å°ãå‡ºã›ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¾ã™ã€‚

[Open in Colab](https://colab.research.google.com/drive/1Fe3LEtkTfLHpGM94gfznYoIhabm_QLY8?usp=sharing)

## å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
# To solve for an error encountered: `NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968`
import locale
locale.getpreferredencoding = lambda: "UTF-8"

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install transformers langchain accelerate bitsandbytes pypdf tiktoken sentence_transformers faiss-gpu trafilatura --quiet
```

```python
# ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã‚„ã™ã„ã‚ˆã†ã«wrapã—ã¦ãŠã
from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)
```

## ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ

å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
from langchain import PromptTemplate
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

## ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’æº–å‚™

ä»Šå›ã¯ã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢ä¸Šã«ã‚ã‚‹ã€ŒONE PIECEã€ã®ãƒšãƒ¼ã‚¸ã‚’ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ã€ãã‚Œã«é–¢é€£ã™ã‚‹è³ªå•ã‚’ã—ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ã€‚

- https://ja.m.wikipedia.org/wiki/ONE_PIECE

ä»Šå›ã¯ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã‚’æŠ½å‡ºã—ã¦ãã‚Œã‚‹ `trafilatura` ã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¾ã—ãŸãŒã€Langchain å†…ã«ã‚‚ã“ã‚Œç”¨ã® `BSHTMLLoader` ã¨ã„ã†ã®ãŒã‚ã‚‹ã‚ˆã†ã§ã™ã€‚ã¾ã è©¦ã›ã¦ã„ã¾ã›ã‚“ã€‚

```python
# https://python.langchain.com/docs/modules/data_connection/document_loaders/html ã‹ã‚‰å¼•ç”¨ã€‚
# from langchain.document_loaders import BSHTMLLoader

# loader = BSHTMLLoader("example_data/fake-content.html")
# data = loader.load()
# data
```

```python
from trafilatura import fetch_url, extract

url = "https://ja.m.wikipedia.org/wiki/ONE_PIECE"
filename = 'textfile.txt'

document = fetch_url(url)
text = extract(document)
print(text[:1000])

with open(filename, 'w', encoding='utf-8') as f:
    f.write(text)
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    ONE PIECE
    ã€ONE PIECEã€ï¼ˆãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹ï¼‰ã¯ã€å°¾ç”°æ „ä¸€éƒã«ã‚ˆã‚‹æ—¥æœ¬ã®å°‘å¹´æ¼«ç”»ä½œå“ã€‚ã€é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—ã€ï¼ˆé›†è‹±ç¤¾ï¼‰ã«ã¦1997å¹´34å·ã‹ã‚‰é€£è¼‰ä¸­ã€‚ç•¥ç§°ã¯ã€Œãƒ¯ãƒ³ãƒ”ã€[3]ã€‚
    |ONE PIECE|
    |ã‚¸ãƒ£ãƒ³ãƒ«||å°‘å¹´æ¼«ç”»ãƒ»æµ·è³Šãƒ»å†’é™º|
    ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ãƒ»ãƒãƒˆãƒ«
    |æ¼«ç”»|
    |ä½œè€…||å°¾ç”°æ „ä¸€éƒ|
    |å‡ºç‰ˆç¤¾||é›†è‹±ç¤¾|
    |
    |
    |æ²è¼‰èªŒ||é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—|
    |ãƒ¬ãƒ¼ãƒ™ãƒ«||ã‚¸ãƒ£ãƒ³ãƒ—ãƒ»ã‚³ãƒŸãƒƒã‚¯ã‚¹|
    |ç™ºè¡¨å·||1997å¹´34å· -|
    |ç™ºè¡¨æœŸé–“||1997å¹´7æœˆ22æ—¥[1] -|
    |å·»æ•°||æ—¢åˆŠ106å·»ï¼ˆ2023å¹´7æœˆ4æ—¥ï¼‰|
    |è©±æ•°||æ—¢åˆŠ1090è©±ï¼ˆ2023å¹´8æœˆ21æ—¥[2]ï¼‰|
    |ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ - ãƒãƒ¼ãƒˆ|
    |ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ||æ¼«ç”»|
    |ãƒãƒ¼ã‚¿ãƒ«||æ¼«ç”»|
    æ¦‚è¦ ç·¨é›†
    æµ·è³Šç‹ã‚’å¤¢è¦‹ã‚‹å°‘å¹´ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ»Dãƒ»ãƒ«ãƒ•ã‚£ã‚’ä¸»äººå…¬ã¨ã™ã‚‹ã€Œã²ã¨ã¤ãªãã®å¤§ç§˜å®ï¼ˆãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹ï¼‰ã€ã‚’å·¡ã‚‹æµ·æ´‹å†’é™ºãƒ­ãƒãƒ³ã€‚
    å¤¢ã¸ã®å†’é™ºãƒ»ä»²é–“ãŸã¡ã¨ã®å‹æƒ…ã¨ã„ã£ãŸãƒ†ãƒ¼ãƒã‚’å‰é¢ã«æ²ã’ã€ãƒãƒˆãƒ«ã‚„ã‚®ãƒ£ã‚°ã‚·ãƒ¼ãƒ³ã€æ„Ÿå‹•ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ¡ã‚¤ãƒ³ã¨ã™ã‚‹å°‘å¹´æ¼«ç”»ã®ç‹é“ã‚’è¡Œãç‰©èªã¨ã—ã¦äººæ°—ã‚’åšã—ã¦ã„ã‚‹[4]ã€‚ã¾ãŸã€é•·å¹´ã«ã‚ãŸã‚ŠãªãŒã‚‰æ·±ãç·´ã‚Šè¾¼ã¾ã‚ŒãŸå£®å¤§ãªä¸–ç•Œè¦³ãƒ»å·§ç·»ãªè¨­å®šã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚‚ç‰¹å¾´ã€‚
    2023å¹´8æœˆã®æ™‚ç‚¹ã§å˜è¡Œæœ¬ã¯ç¬¬106å·»ã¾ã§åˆŠè¡Œã•ã‚Œã¦ãŠã‚Šã€ã€é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—ã€æ­´ä»£ä½œå“ã®ä¸­ã§ã¯ã€ã“ã¡ã‚‰è‘›é£¾åŒºäº€æœ‰å…¬åœ’å‰æ´¾å‡ºæ‰€ã€ï¼ˆ1976å¹´ - 2016å¹´ï¼‰ã«æ¬¡ãé•·æœŸé€£è¼‰ã¨ãªã£ã¦ã„ã‚‹ã€‚å›½å†…ç´¯è¨ˆç™ºè¡Œéƒ¨æ•°ã¯2022å¹´æ™‚ç‚¹ã§æ—¥æœ¬ã®æ¼«ç”»ã§ã¯æœ€é«˜ã¨ãªã‚‹4å„„1000ä¸‡éƒ¨ã‚’çªç ´ã—ã¦ã„ã‚‹[5]ã€‚ã¾ãŸç¬¬67å·»ã¯åˆç‰ˆç™ºè¡Œéƒ¨æ•°405ä¸‡éƒ¨[6]ã®å›½å†…å‡ºç‰ˆå²ä¸Šæœ€é«˜è¨˜éŒ²ã‚’æ¨¹ç«‹ã—ã€ç¬¬57å·»ï¼ˆ2010å¹´3æœˆç™ºå£²ï¼‰ä»¥é™ã®å˜è¡Œæœ¬ã¯åˆç‰ˆ300ä¸‡éƒ¨ä»¥ä¸Šç™ºè¡Œã‚’ç¶™ç¶šã™ã‚‹[7]ãªã©å‡ºç‰ˆã®å›½å†…æœ€é«˜è¨˜éŒ²ã‚’ã„ãã¤ã‚‚ä¿æŒã—ã¦ã„ã‚‹ã€‚
    2015å¹´6æœˆ15æ—¥ã«ã¯ "Most Copies Published For The Same Comic Book Series By A Single Authorï¼ˆæœ€ã‚‚å¤šãç™ºè¡Œã•ã‚ŒãŸå˜ä¸€ä½œè€…ã«ã‚ˆã‚‹ã‚³ãƒŸãƒƒã‚¯ã‚·ãƒªãƒ¼ã‚ºï¼‰" åç¾©ã§ã‚®ãƒã‚¹ä¸–ç•Œè¨˜éŒ²ã«èªå®šã•ã‚ŒãŸ[8][9]ã€‚å®Ÿç¸¾ã¯ç™ºè¡Œéƒ¨æ•°3å„„2,086ä¸‡6,000éƒ¨ï¼ˆ2014å¹´12æœˆæ™‚ç‚¹ï¼‰[8]ã€‚ãªãŠã“ã®ã‚®ãƒã‚¹ä¸–ç•Œè¨˜éŒ²ã¯2022å¹´7æœˆä»˜ã§åŒä½œå“ã«ã‚ˆã£ã¦æ›´æ–°ã•ã‚Œ[10]ã€æ—¥æœ¬ã§ã¯åŒå¹´8æœˆã«ã€Œæ—¥æœ¬å›½å†…ç´¯è¨ˆç™ºè¡Œéƒ¨æ•°4å„„1656ä¸‡

æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜å‡ºæ¥ã¾ã—ãŸã€‚ã“ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Langchain ã® TextSplitter ã‚’ä½¿ã£ã¦å°å£ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ‡ã£ã¦ã„ãã¾ã™ã€‚

ã“ã†ã—ã¦ç”Ÿæˆã—ãŸãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ embedding ã‚’ç”Ÿæˆã—ã€è³ªå•ã® embedding ã«ä¸€ç•ªè¿‘ã„ãƒˆãƒƒãƒ—ï½‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡ºã€‚ãã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã«çªã£è¾¼ã¿ã€è³ªå•ã¨åŒæ™‚ã« LLM ã«æŠ•ã’ã¦å›ç­”ã‚’å¾—ã‚‹ã€‚ã¨ã„ã£ãŸæµã‚Œã¨ãªã‚Šã¾ã™ã€‚

ç§ã¯å°‘ãªãã¨ã‚‚ãã†ã„ã†ç†è§£ã§ã™ã€‚

```python
loader = TextLoader(filename, encoding='utf-8')
documents = loader.load()

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator = "\n",
    chunk_size=300,
    chunk_overlap=20,
)
texts = text_splitter.split_documents(documents)
print(len(texts))
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    WARNING:langchain.text_splitter:Created a chunk of size 361, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 388, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 333, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 301, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 336, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 540, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 464, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 366, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 331, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 327, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 409, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 442, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 389, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 425, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 370, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 399, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 345, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 531, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 390, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 504, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 567, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 478, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 466, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 383, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 374, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 347, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 492, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 435, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 350, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 416, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 331, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 453, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 319, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 763, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 417, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 372, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 450, which is longer than the specified 300
    WARNING:langchain.text_splitter:Created a chunk of size 358, which is longer than the specified 300


    493

ã©ã‚“ãªæ§‹é€ ã‚’ã—ã¦ã„ã‚‹ã®ã‹çŸ¥ã‚‹ãŸã‚ã«ã€ä½•å€‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
texts[30:33]
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    [Document(page_content='ã‚¢ãƒ©ãƒã‚¹ã‚¿ç·¨ ç·¨é›†\n- ã€12å·» - 23å·»ã€‘\n-\n- å‰å¤§ãªã‚‹èˆªè·¯çªå…¥ï¼ˆ12å·»ï¼‰\n- éº¦ã‚ã‚‰ã®ä¸€å‘³ã¯ã¤ã„ã«ã€Œå‰å¤§ãªã‚‹èˆªè·¯ã€ã«çªå…¥ã™ã‚‹ã€‚ãƒªãƒ´ã‚¡ãƒ¼ã‚¹ãƒ»ãƒã‚¦ãƒ³ãƒ†ãƒ³ã‚’é™ã‚ŠãŸå ´æ‰€ã«ã‚ã‚‹ã€ŒåŒå­å²¬ã€ã§ã€ä»²é–“ã®å¸°é‚„ã‚’å¾…ã¡ç¶šã‘ã‚‹ã‚¯ã‚¸ãƒ©ãƒ»ãƒ©ãƒ–ãƒ¼ãƒ³ã¨å‡ºä¼šã†ã€‚ãƒ«ãƒ•ã‚£ã¯ãƒ©ãƒ–ãƒ¼ãƒ³ã¨ã€ã€Œå‰å¤§ãªã‚‹èˆªè·¯ã€ä¸€å‘¨å¾Œã«å†æˆ¦ã™ã‚‹ç´„æŸã‚’äº¤ã‚ã™ã€‚\n- ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ãƒ”ãƒ¼ã‚¯ç·¨ï¼ˆ12å·» - 13å·»ï¼‰', metadata={'source': 'textfile.txt'}),
     Document(page_content='- ãƒ«ãƒ•ã‚£é”ã¯ã€æœ€åˆã®å³¶ã€Œã‚µãƒœãƒ†ãƒ³å³¶ã€ã®ç”ºã€Œã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ãƒ”ãƒ¼ã‚¯ã€ã§å¤§æ­“è¿ã‚’å—ã‘ã‚‹ã€‚ã ãŒãã®ç”ºã¯ã€ç§˜å¯†çŠ¯ç½ªä¼šç¤¾ã€Œãƒãƒ­ãƒƒã‚¯ãƒ¯ãƒ¼ã‚¯ã‚¹ã€ï¼ˆBãƒ»Wï¼‰ã®ç¤¾å“¡ã§ã‚ã‚‹è³é‡‘ç¨¼ãé”ã®å·£ã§ã‚ã£ãŸã€‚ãã“ã§ä¸€å‘³ã¯ã€Bãƒ»Wã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸€äººã®æ­£ä½“ãŒã€ã€Œå‰å¤§ãªã‚‹èˆªè·¯ã€ã«ã‚ã‚‹å¤§å›½ã€Œã‚¢ãƒ©ãƒã‚¹ã‚¿ç‹å›½ã€ã®ç‹å¥³ãƒãƒ•ã‚§ãƒ«ã‚¿ãƒªãƒ»ãƒ“ãƒ“ã§ã‚ã‚‹ã¨çŸ¥ã‚‹ã€‚Bãƒ»Wã«æ½œå…¥ã—ã¦ã„ãŸå½¼å¥³ã‹ã‚‰ã€ãƒ«ãƒ•ã‚£é”ã¯Bãƒ»Wã«ã‚ˆã‚‹ã‚¢ãƒ©ãƒã‚¹ã‚¿ç‹å›½ä¹—ã£å–ã‚Šè¨ˆç”»ã‚’çŸ¥ã‚‹ã€‚ãƒ“ãƒ“ã‚’ä¸€è¡Œã«åŠ ãˆãŸéº¦ã‚ã‚‰ã®ä¸€å‘³ã¯ã€Bãƒ»Wã‹ã‚‰ã®è¿½æ‰‹ã‚’æŒ¯ã‚Šåˆ‡ã‚Šã¤ã¤ã€è¨ˆç”»ã‚’é˜»æ­¢ã™ã¹ãä¸€è·¯ã‚¢ãƒ©ãƒã‚¹ã‚¿ã‚’ç›®æŒ‡ã™ã€‚', metadata={'source': 'textfile.txt'}),
     Document(page_content='- ãƒªãƒˆãƒ«ã‚¬ãƒ¼ãƒ‡ãƒ³ç·¨ï¼ˆ13å·» - 15å·»ï¼‰\n- ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ãƒ”ãƒ¼ã‚¯ã‚’å‡ºæ¸¯ã—ãŸãƒ«ãƒ•ã‚£é”ã¯ã€ã‚¸ãƒ£ãƒ³ã‚°ãƒ«ã®ä¸­ã§æç«œé”ãŒç”Ÿãã‚‹å¤ªå¤ã®å³¶ã€Œãƒªãƒˆãƒ«ã‚¬ãƒ¼ãƒ‡ãƒ³ã€ã«ä¸Šé™¸ã™ã‚‹ã€‚ãƒ«ãƒ•ã‚£é”ã¯ãã®å³¶ã§ã€å·¨äººæ—ã®äºŒäººã®æˆ¦å£«ãƒ»ãƒ‰ãƒªãƒ¼ã¨ãƒ–ãƒ­ã‚®ãƒ¼ã«å‡ºä¼šã†ã€‚å½¼ã‚‰ã¯ã€Œèª‡ã‚Šã€ã‚’å®ˆã‚‹ãŸã‚ã€100å¹´é–“ã‚‚æ±ºé—˜ã‚’ç¶šã‘ã¦ããŸã¨ã„ã†ã€‚ã ãŒãã®æ±ºé—˜ãŒã€Bãƒ»Wã‹ã‚‰ã®è¿½æ‰‹ã«ã‚ˆã‚‹å‘åŠ£ãªç­–ç•¥ã§é‚ªé­”ã•ã‚Œã‚‹ã€‚ãƒ«ãƒ•ã‚£é”ã¯Bãƒ»Wã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã—ã¦å§‘æ¯ãªç¾è¡“å®¶ãƒ»Mr.3ã‚‰ã‚’ç ´ã‚Šã€å·¨äººæ—ã®èª‡ã‚Šã‚’å®ˆã‚‹ã€‚\n- ãƒ‰ãƒ©ãƒ å³¶ç·¨ï¼ˆ15å·» - 17å·»ï¼‰', metadata={'source': 'textfile.txt'})]

## Embedding ã®ç”Ÿæˆã¨ FAISS ã‚’ä½¿ã£ãŸãƒ™ã‚¯ãƒˆãƒ« DB ã®ç”¨æ„

å°å£ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ‡ã£ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Embedding ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ embedding ã«å¤‰æ›ã—ã¦ã„ãã¾ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼æ€§ã‚’ã‚‚ã¨ã«æ¤œç´¢ã‚’ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãŸã‚ã§ã™ã€‚

Embedding ã®ç”Ÿæˆã«ã¯ `intfloat/multilingual-e5-large` ã‚’ä½¿ã„ã¾ã™ã€‚

ãƒ™ã‚¯ãƒˆãƒ« DB ã«ã¯ `FAISS` ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ç”¨ã„ã¾ã™ã€‚ï¼ˆä»Šå›ã¯ GPU ã®ã‚ã‚‹ç’°å¢ƒã§èµ°ã‚‰ã›ã¦ã¿ã¦ã„ã‚‹ãŸã‚ã€ faiss-gpu ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ã€‚ï¼‰

```python
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
db = FAISS.from_documents(texts, embeddings)

# ä¸€ç•ªé¡ä¼¼ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’ã„ãã¤ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã‚’å¤‰æ•°kã«è¨­å®šå‡ºæ¥ã¾ã™ã€‚
retriever = db.as_retriever(search_kwargs={"k": 3})
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

ä»Šå›ã®ç’°å¢ƒã§ã¯ embedding ã‚’ç”¨æ„ã™ã‚‹ã®ã« 25 ç§’ã»ã©ã‹ã‹ã‚Šã¾ã—ãŸã€‚

## ãƒ¢ãƒ‡ãƒ«ã®ç”¨æ„

ä»Šå›ã¯ Elyza-7b-instruct ã‚’ç”¨ã„ã¾ã™ã€‚

ä»Šå›ã¯ Colab ã® T4 ã§ã‚‚å•é¡Œãªãå®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã€BitsandBytes ã§ 4bit ã«é‡å­åŒ–ã—ãŸã‚‚ã®ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = "elyza/ELYZA-japanese-Llama-2-7b-instruct"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=quantization_config,
).eval()
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]

æ¬¡ã«ã€Elyza-7b-instruct ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨æ„ã—ã¾ã™ã€‚

```python
B_INST, E_INST = "[INST]", "[/INST]"
B_SYS, E_SYS = "<<SYS>>\n", "\n<</SYS>>\n\n"
DEFAULT_SYSTEM_PROMPT = "å‚è€ƒæƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ã§ãã‚‹ã ã‘æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚"
text = "{context}\nãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚{question}"
template = "{bos_token}{b_inst} {system}{prompt} {e_inst} ".format(
    bos_token=tokenizer.bos_token,
    b_inst=B_INST,
    system=f"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}",
    prompt=text,
    e_inst=E_INST,
)
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

## LLM ã¨ Chain ã®æŒ‡å®š

```python
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
)
PROMPT = PromptTemplate(
    template=template,
    input_variables=["question","context"],
    template_format="f-string"
)

chain_type_kwargs = {"prompt": PROMPT}

qa = RetrievalQA.from_chain_type(
    llm=HuggingFacePipeline(
        pipeline=pipe,
        # model_kwargs=dict(temperature=0.1, do_sample=True, repetition_penalty=1.1)
    ),
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs,
    verbose=True,
)
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

## ãŠè©¦ã—

è¦ç´„è³ªå•ãŒã§ãã‚‹çŠ¶æ…‹ãŒæ•´ã„ã¾ã—ãŸã€‚
æœ€åˆã« RAG ã‚’ä½¿ã‚ãšã« LLM ã«è³ªå•ã‚’ã—ã€ãã®å¾Œã« RAG ã‚’ä½¿ã£ã¦ç”Ÿæˆã—ã¦ã¿ã¦å·®åˆ†ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
inputs = template.format(context='', question='ãƒ‹ã‚³ãƒ»ãƒ­ãƒ“ãƒ³ã®è·æ¥­ã¯ä½•ã§ã™ã‹ï¼Ÿ')
inputs = tokenizer(inputs, return_tensors='pt').to(model.device)

with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)
output
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    '[INST] <<SYS>>\nå‚è€ƒæƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ã§ãã‚‹ã ã‘æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚\n<</SYS>>\n\n\nãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚ãƒ‹ã‚³ãƒ»ãƒ­ãƒ“ãƒ³ã®è·æ¥­ã¯ä½•ã§ã™ã‹ï¼Ÿ [/INST]  ãƒ‹ã‚³ãƒ»ãƒ­ãƒ“ãƒ³ã®è·æ¥­ã¯ã€å¼·ç›—å›£ã®ä¸€å“¡ã§ã™ã€‚'

```python
result = qa("ãƒ‹ã‚³ãƒ»ãƒ­ãƒ“ãƒ³ã®è·æ¥­ã¯ä½•ã§ã™ã‹ï¼Ÿ")
print('å›ç­”:', result['result'])
print('='*10)
print('ã‚½ãƒ¼ã‚¹:', result['source_documents'])
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    [1m> Entering new RetrievalQA chain...[0m

    [1m> Finished chain.[0m
    å›ç­”:  ãƒ‹ã‚³ãƒ»ãƒ­ãƒ“ãƒ³ã®è·æ¥­ã¯ã€Œè€ƒå¤å­¦è€…ã€ã§ã™ã€‚
    ==========
    ã‚½ãƒ¼ã‚¹: [Document(page_content='ç©ºå³¶ç·¨ ç·¨é›†\n- ã€24å·» - 32å·»ã€‘\n-\n- ã‚¸ãƒ£ãƒ¤ç·¨ï¼ˆ24å·» - 25å·»ï¼‰\n- ã‚¢ãƒ©ãƒã‚¹ã‚¿ã‚’å¾Œã«ã—ãŸãƒ«ãƒ•ã‚£é”ã¯ã€Bãƒ»Wç¤¾å‰¯ç¤¾é•·ã§ã‚ã£ãŸè€ƒå¤å­¦è€…ãƒ‹ã‚³ãƒ»ãƒ­ãƒ“ãƒ³ã‚’ä»²é–“ã«åŠ ãˆã‚‹ã€‚æ¬¡ã®å³¶ã«å‘ã‹ã†èˆªæµ·ä¸­ã€çªå¦‚ç©ºã‹ã‚‰å·¨å¤§ãªã‚¬ãƒ¬ã‚ªãƒ³èˆ¹ãŒè½ä¸‹ã—ã€ã€Œè¨˜éŒ²æŒ‡é‡ï¼ˆãƒ­ã‚°ãƒãƒ¼ã‚¹ï¼‰ã€ã®æŒ‡ã™é€²è·¯ãŒä¸Šå‘ãã«å¤‰æ›´ã•ã‚Œã‚‹ã€‚ãã‚Œã¯ä¼èª¬ã¨ã•ã‚Œã‚‹ç©ºã«æµ®ã‹ã¶å³¶ã€Œç©ºå³¶ã€ã¸ã®æŒ‡é‡ã‚’æ„å‘³ã—ã¦ã„ãŸã€‚', metadata={'source': 'textfile.txt'}), Document(page_content='- ã€ŒTHE 8TH LOG "SKYPIEA"ã€2008å¹´4æœˆç™ºè¡Œã€ISBN 978-4-08-111027-8\n- ã€ŒTHE 9TH LOG "GOD"ã€2008å¹´5æœˆç™ºè¡Œã€ISBN 978-4-08-111028-5\n- ã€ŒTHE 10TH LOG "BELL"ã€2008å¹´6æœˆç™ºè¡Œã€ISBN 978-4-08-111029-2\n- ã€ŒTHE 11TH LOG "WATER SEVEN"ã€2009å¹´4æœˆç™ºè¡Œã€ISBN 978-4-08-111009-4\n- ã€ŒTHE 12TH LOG "ROCKET MAN"ã€2009å¹´5æœˆç™ºè¡Œã€ISBN 978-4-08-111010-0\n- ã€ŒTHE 13TH LOG "NICO ROBIN"ã€2009å¹´7æœˆç™ºè¡Œã€ISBN 978-4-08-111011-7\n- ã€ŒTHE 14TH LOG "FRANKY"ã€2009å¹´8æœˆç™ºè¡Œã€ISBN 978-4-08-111012-4', metadata={'source': 'textfile.txt'}), Document(page_content='- å£° - å¤§è°·è‚²æ±Ÿ\n- éº¦ã‚ã‚‰ã®ä¸€å‘³èˆ¹åŒ»ã€‚ã€Œãƒ’ãƒˆãƒ’ãƒˆã®å®Ÿã€ã‚’é£Ÿã¹äººã®èƒ½åŠ›ã‚’æŒã£ãŸäººé–“ãƒˆãƒŠã‚«ã‚¤ã€‚ä¸‡èƒ½è–¬ï¼ˆä½•ã§ã‚‚æ²»ã›ã‚‹åŒ»è€…ï¼‰ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚\n- ãƒ‹ã‚³ãƒ»ãƒ­ãƒ“ãƒ³\n- å£° - å±±å£ç”±é‡Œå­\n- éº¦ã‚ã‚‰ã®ä¸€å‘³è€ƒå¤å­¦è€…ã€‚ã€ŒãƒãƒŠãƒãƒŠã®å®Ÿã€ã®èƒ½åŠ›è€…ã€‚æ­´å²ä¸Šã®ã€Œç©ºç™½ã®100å¹´ã€ã®è¬ã‚’è§£ãæ˜ã‹ã™ãŸã‚æ—…ã‚’ã—ã¦ã„ã‚‹ã€‚\n- ãƒ•ãƒ©ãƒ³ã‚­ãƒ¼\n- å£° - çŸ¢å°¾ä¸€æ¨¹', metadata={'source': 'textfile.txt'})]

```python
inputs = template.format(context='', question='ã‚¨ãƒãƒ«ã¯ä½•è€…ã§ã™ã‹ï¼Ÿ')
inputs = tokenizer(inputs, return_tensors='pt').to(model.device)

with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)
output
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    '[INST] <<SYS>>\nå‚è€ƒæƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ã§ãã‚‹ã ã‘æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚\n<</SYS>>\n\n\nãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚ã‚¨ãƒãƒ«ã¯ä½•è€…ã§ã™ã‹ï¼Ÿ [/INST]  ã‚¨ãƒãƒ«ã¯ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ã“ã¨ã§ã™ã€‚'

```python
result = qa("ã‚¨ãƒãƒ«ã¯ä½•è€…ã§ã™ã‹ï¼Ÿ")
print('å›ç­”:', result['result'])
print('='*10)
print('ã‚½ãƒ¼ã‚¹:', result['source_documents'])
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    [1m> Entering new RetrievalQA chain...[0m

    [1m> Finished chain.[0m
    å›ç­”:  ã‚¨ãƒãƒ«ã¯ã€ONE PIECEã®ç™»å ´äººç‰©ã§ã‚ã‚‹ç¥ã®å›½ã€Œã‚¹ã‚«ã‚¤ãƒ”ã‚¢ã€ã®ç¥ã®ä¸€äººã§ã™ã€‚ç¥ã®å›½ã€Œã‚¹ã‚«ã‚¤ãƒ”ã‚¢ã€ã¯ã€ã‹ã¤ã¦åœ°ä¸Šã«å­˜åœ¨ã—ãŸä¼èª¬ã®é»„é‡‘éƒ·ã§ã‚ã‚‹ã€Œç¥ã®å³¶ï¼ˆã‚¢ãƒƒãƒ‘ãƒ¼ãƒ¤ãƒ¼ãƒ‰ï¼‰ã€ãŒã€ã‹ã‚‰æ”¯é…ã—ã¦ã„ã¾ã™ã€‚ã‚¨ãƒãƒ«ã¯ã€ç¥ã®è»å›£ã‚’ç‡ã„ã¦ã„ã¾ã™ã€‚
    ==========
    ã‚½ãƒ¼ã‚¹: [Document(page_content='- ãƒ«ãƒ•ã‚£é”ã¯ä¸Šç©º1ä¸‡ãƒ¡ãƒ¼ãƒˆãƒ«ã«ã‚ã‚‹ç©ºå³¶ã«è¾¿ã‚Šç€ãã€‚ãã“ã«ã¯ä»Šã¾ã§å…¨ãè¦‹ãŸã“ã¨ãŒãªã„æœªçŸ¥ã®æ–‡åŒ–ãŒåºƒãŒã£ã¦ã„ãŸã€‚ãƒ«ãƒ•ã‚£é”ã¯ã€ç¥ã®å›½ã€Œã‚¹ã‚«ã‚¤ãƒ”ã‚¢ã€ã§ä¸Šé™¸ã—ãŸã€Œç¥ã®å³¶ï¼ˆã‚¢ãƒƒãƒ‘ãƒ¼ãƒ¤ãƒ¼ãƒ‰ï¼‰ã€ãŒã€ã‹ã¤ã¦åœ°ä¸Šã«å­˜åœ¨ã—ãŸä¼èª¬ã®é»„é‡‘éƒ·ã§ã‚ã‚‹ã“ã¨ã‚’ã¤ãã¨ã‚ã‚‹ã€‚ã—ã‹ã—ã€ãã“ã¯ç¥ã®è»å›£ã‚’ç‡ã„ã‚‹ã€ç¥ãƒ»ã‚¨ãƒãƒ«ã€ŸãŒæ”¯é…ã™ã‚‹åœŸåœ°ã§ã‚ã‚Šã€ç©ºã®æ°‘ã¨å³¶ã®å…ˆä½æ°‘ã‚·ãƒ£ãƒ³ãƒ‡ã‚£ã‚¢ãŒ400å¹´ã«æ¸¡ã‚Šäº‰ã„ç¶šã‘ã¦ã„ã‚‹åœŸåœ°ã§ã‚ã£ãŸã€‚é»„é‡‘æœã—ã«ä¹—ã‚Šå‡ºã—ãŸãƒ«ãƒ•ã‚£é”ã¯ã€ç¥ã®è»å›£ã¨ã‚·ãƒ£ãƒ³ãƒ‡ã‚£ã‚¢ã¨ã®éé…·ãªã‚µãƒã‚¤ãƒãƒ«ã«å·»ãè¾¼ã¾ã‚Œã‚‹ã€‚ã‚¨ãƒãƒ«ã®åœ§å€’çš„ãªåŠ›ã«å¤šãã®æˆ¦å£«ãŸã¡ãŒå€’ã‚Œã¦ã„ãã€ã‚¨ãƒãƒ«ã«ã‚ˆã£ã¦ç©ºå³¶ã¯æ¶ˆæ»…ã®å±æ©Ÿã«é™¥ã‚‹ã€‚ã ãŒã€å”¯ä¸€ã‚¨ãƒãƒ«ã«å¯¾æŠ—ã§ãã‚‹ãƒ«ãƒ•ã‚£ã«ã‚ˆã£ã¦ç©ºå³¶ã®å±æ©Ÿã¯é˜²ãŒã‚Œã€400å¹´ã«æ¸¡ã‚‹ç©ºã®æ°‘ã¨ã‚·ãƒ£ãƒ³ãƒ‡ã‚£ã‚¢ã®äº‰ã„ã«çµ‚æ­¢ç¬¦ãŒæ‰“ãŸã‚ŒãŸã€‚', metadata={'source': 'textfile.txt'}), Document(page_content='- ä¸€æ–¹ã€ã‚µãƒ‹ãƒ¼å·ã¯å·¨å¤§ãªãƒ­ãƒœãƒƒãƒˆã«æ•ã¾ã£ã¦ã‚¨ãƒƒã‚°ãƒ˜ãƒƒãƒ‰ã«é€£è¡Œã•ã‚Œã‚‹ã€‚ã‚¾ãƒ­ãŸã¡ã¯ãƒ™ã‚¬ãƒ‘ãƒ³ã‚¯ã®åˆ†èº«ã®ã€Œæ‚ª(ãƒªãƒªã‚¹)ã€ã¨ã€Œæ­£(ã‚·ãƒ£ã‚«)ã€ã«ã‚ˆã‚Šã€ç ”ç©¶æ‰€ã«é€šã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ã€‚ç ”ç©¶æ‰€ã«ç€ãã¨ã€ä¸€å‘³ã¯ã‚¸ãƒ³ãƒ™ã‚¨ãã£ãã‚Šã®æ–°å‹ãƒ‘ã‚·ãƒ•ã‚£ã‚¹ã‚¿ã€Œã‚»ãƒ©ãƒ•ã‚£ãƒ ã€ã®è¥²æ’ƒã‚’å—ã‘æˆ¦é—˜ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã‚‹ãŒã€ã‚»ãƒ©ãƒ•ã‚£ãƒ ã‚’ç ´å£Šã•ã‚Œã‚‹å‰ã«æ­£(ã‚·ãƒ£ã‚«)ãŒæˆ¦é—˜ã‚’ä¸­æ­¢ã•ã›ã‚‹ã€‚æ­£(ã‚·ãƒ£ã‚«)ã¯ã€ã“ã®å³¶ãŒã€Œéå»ã€ã§ã‚ã‚Šã€ã“ã®å³¶ã®ã‚ˆã†ãªé«˜åº¦ãªæ–‡æ˜ã‚’æŒã£ãŸç‹å›½ãŒ900å¹´å‰ã«å®Ÿåœ¨ã—ã¦ã„ãŸã¨èªã‚‹ã€‚', metadata={'source': 'textfile.txt'}), Document(page_content='ãƒãƒ™ãƒ©ã‚¤ã‚ºä½œå“ ç·¨é›†\né›†è‹±ç¤¾ã®æ–°æ›¸ãƒ¬ãƒ¼ãƒ™ãƒ«ã€ŒJUMP j BOOKSã€ã‚ˆã‚Šç™ºå£²ã•ã‚Œã¦ã„ã‚‹ã€ã‚¢ãƒ‹ãƒ¡ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚„åŠ‡å ´ç‰ˆã®ãƒãƒ™ãƒ©ã‚¤ã‚ºä½œå“ã€‚ä¸€éƒ¨ã¯å…ç«¥æ–‡å­¦ãƒ¬ãƒ¼ãƒ™ãƒ«ã€Œé›†è‹±ç¤¾ã¿ã‚‰ã„æ–‡åº«ã€ã§ã‚‚åˆŠè¡Œã•ã‚Œã¦ã„ã‚‹ã€‚\nãã®ä»–ã®å°èª¬ä½œå“ ç·¨é›†\n- ONE PIECE novel Aï¼ˆã‚¨ãƒ¼ã‚¹ï¼‰\n- ã‚¨ãƒ¼ã‚¹ã‚’ä¸»äººå…¬ã¨ã—ã€ã‚¹ãƒšãƒ¼ãƒ‰æµ·è³Šå›£æ™‚ä»£ã®å†’é™ºã‚’æãã€‚ãƒ ãƒƒã‚¯ã€ONE PIECE magazineã€Vol.1ã‹ã‚‰Vol.3ã¾ã§é€£è¼‰ã•ã‚Œ[162][163]ã€å¾Œã«ç¬¬1å·»ã¨ã—ã¦æ›¸ç±åŒ–ã•ã‚ŒãŸã€‚è‘—è€…ã¯ã²ãªãŸã—ã‚‡ã†ã€‚', metadata={'source': 'textfile.txt'})]

```python
inputs = template.format(context='', question='ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã®ç‰¹æ®Šèƒ½åŠ›ã¯ä½•ã§ã™ã‹ï¼Ÿ')
inputs = tokenizer(inputs, return_tensors='pt').to(model.device)

with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)
output
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    '[INST] <<SYS>>\nå‚è€ƒæƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ã§ãã‚‹ã ã‘æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚\n<</SYS>>\n\n\nãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã®ç‰¹æ®Šèƒ½åŠ›ã¯ä½•ã§ã™ã‹ï¼Ÿ [/INST]  ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã®ç‰¹æ®Šèƒ½åŠ›ã«ã¤ã„ã¦å›ç­”ã„ãŸã—ã¾ã™ã€‚\n\nãƒãƒ§ãƒƒãƒ‘ãƒ¼ã¯ã€ç›¸æ‰‹ã®æ”»æ’ƒã‚’å—ã‘ã¦ã‚‚ãã®æ”»æ’ƒã‚’ç›¸æ‰‹ã«è¿”ã—ã¦ãã‚‹ã“ã¨ã®ã§ãã‚‹ã€Œãƒªãƒ•ãƒ¬ã‚¯ã‚¿ãƒ¼ã€ã¨ã„ã†èƒ½åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã¯æ”»æ’ƒã‚’å—ã‘ã‚‹ã“ã¨ãŒå¤šãã€å®ˆå‚™åŠ›ãŒä½ã„ã¨ã„ã†å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚'

```python
result = qa("ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã®ç‰¹æ®Šèƒ½åŠ›ã¯ä½•ã§ã™ã‹ï¼Ÿ")
print('å›ç­”:', result['result'])
print('='*10)
print('ã‚½ãƒ¼ã‚¹:', result['source_documents'])
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    [1m> Entering new RetrievalQA chain...[0m

    [1m> Finished chain.[0m
    å›ç­”:  ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã®ç‰¹æ®Šèƒ½åŠ›ã¯ã€äººã®èƒ½åŠ›ã‚’æŒã¤ãƒˆãƒŠã‚«ã‚¤ã§ã‚ã‚‹ãŸã‚ã€ãã®èƒ½åŠ›ã¯ã€Œäººã®ã‚ˆã†ã«æ­©ãã€ã“ã¨ã§ã™ã€‚

    ã¾ãŸã€ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã¯å…ƒã€…äººé–“ã§ã¯ãªãã€æ‚ªé­”ã®å®Ÿã®èƒ½åŠ›ã§äººã®ã‚ˆã†ã«æ­©ãèƒ½åŠ›ã‚’å¾—ãŸãƒˆãƒŠã‚«ã‚¤ã§ã™ã€‚
    ==========
    ã‚½ãƒ¼ã‚¹: [Document(page_content='- ãƒãƒ§ãƒƒãƒ‘ãƒ¼ãƒãƒ³\n- ãƒ‘ãƒ©ãƒ¬ãƒ«ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚’èˆå°ã«ã€ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã‚’ä¸»äººå…¬ã«ã—ãŸã‚¹ãƒ”ãƒ³ã‚ªãƒ•æ¼«ç”»ã€‚ä½œç”»ã¯æ­¦äº•å®æ–‡ã€‚ã€æœ€å¼·ã‚¸ãƒ£ãƒ³ãƒ—ã€2012å¹´1æœˆå·ã‹ã‚‰2014å¹´2æœˆå·ã¾ã§é€£è¼‰ã•ã‚ŒãŸã€‚\n- ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼\n- SDåŒ–ã—ãŸã‚­ãƒ£ãƒ©ãŸã¡ãŒç¹°ã‚Šåºƒã’ã‚‹ã€ã‚¹ãƒ”ãƒ³ã‚ªãƒ•ã‚®ãƒ£ã‚°æ¼«ç”»ã€‚ä½œç”»ã¯å®‰è—¤è‹±ã€‚ã€æœ€å¼·ã‚¸ãƒ£ãƒ³ãƒ—ã€2015å¹´1æœˆå·ã‚ˆã‚Šé€£è¼‰ä¸­ã€‚\n- CHIN PIECE[38]', metadata={'source': 'textfile.txt'}), Document(page_content='- 11æœˆ11æ—¥ - å˜è¡Œæœ¬å›½å†…ç´¯è¨ˆç™ºè¡Œéƒ¨æ•°ãŒ2å„„å†Šã‚’çªç ´ï¼ˆç¬¬60å·»ï¼‰[15]ã€‚\n- 2011å¹´ï¼ˆå¹³æˆ23å¹´ï¼‰\n- 4æœˆ - ã€é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ— 2011å¹´4æœˆ4æ—¥å· No.17ã€ã«å³¶è¢‹å…‰å¹´ã®ã€ãƒˆãƒªã‚³ã€ã¨ã®ã‚¯ãƒ­ã‚¹ã‚ªãƒ¼ãƒãƒ¼ä½œå“ã€å®Ÿé£Ÿ! æ‚ªé­”ã®å®Ÿ!!ã€ãŒæ²è¼‰ã•ã‚Œã‚‹ã€‚\n- 12æœˆ3æ—¥ - ã€æœ€å¼·ã‚¸ãƒ£ãƒ³ãƒ— 2012å¹´1æœˆå·ï¼ˆ2011å¹´12æœˆ3æ—¥ç™ºå£²å·ï¼‰ã€ã‚ˆã‚Šã€ã‚¹ãƒ”ãƒ³ã‚ªãƒ•æ¼«ç”»ã€ãƒãƒ§ãƒƒãƒ‘ãƒ¼ãƒãƒ³ã€ãŒé€£è¼‰é–‹å§‹ã€‚\n- 2012å¹´ï¼ˆå¹³æˆ24å¹´ï¼‰', metadata={'source': 'textfile.txt'}), Document(page_content='- ãƒªãƒˆãƒ«ã‚¬ãƒ¼ãƒ‡ãƒ³å‡ºæ¸¯å¾Œã€ãƒŠãƒŸãŒæ€¥ç—…ã«å€’ã‚Œã¦ã—ã¾ã†ã€‚æ€¥é½é€²è·¯ã‚’å¤‰æ›´ã—ã€é›ªã®å³¶ã€Œãƒ‰ãƒ©ãƒ å³¶ã€ã«ç«‹ã¡å¯„ã£ãŸéº¦ã‚ã‚‰ã®ä¸€å‘³ã¯ã€æ‚ªé­”ã®å®Ÿã‚’é£Ÿã¹äººã®èƒ½åŠ›ã‚’æŒã£ãŸãƒˆãƒŠã‚«ã‚¤ã€ãƒˆãƒ‹ãƒ¼ãƒˆãƒ‹ãƒ¼ãƒ»ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã¨å‡ºä¼šã†ã€‚ãƒ«ãƒ•ã‚£ã¯ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã‚’ä»²é–“ã«èª˜ã†ãŒã€å½¼ã«ã¯æ‚²ã—ãéå»ãŒã‚ã£ãŸã€‚ãã“ã¸ã€ã‹ã¤ã¦å³¶ã§æ‚ªæ”¿ã‚’æ•·ã„ãŸå…ƒãƒ‰ãƒ©ãƒ ç‹å›½å›½ç‹ãƒ¯ãƒãƒ«ãŒå¸°é‚„ã™ã‚‹ã€‚ãƒ«ãƒ•ã‚£é”ã¯ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã¨å…±é—˜ã—ã¦ãƒ¯ãƒãƒ«ã‚’æ’ƒé€€ã—ã€èˆ¹åŒ»ãƒãƒ§ãƒƒãƒ‘ãƒ¼ã‚’ä»²é–“ã«è¿ãˆã‚‹ã€‚\n- ã‚¢ãƒ©ãƒã‚¹ã‚¿ç·¨ï¼ˆ17å·» - 23å·»ï¼‰', metadata={'source': 'textfile.txt'})]

```python
inputs = template.format(context='', question="ã€ŒONE PIECEã€ã¨ã¯ä½œå“ã®ä¸­ã§ä½•ã‚’æŒ‡ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ")
inputs = tokenizer(inputs, return_tensors='pt').to(model.device)

with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)
output
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    '[INST] <<SYS>>\nå‚è€ƒæƒ…å ±ã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ã§ãã‚‹ã ã‘æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚\n<</SYS>>\n\n\nãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è³ªå•ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚ã€ŒONE PIECEã€ã¨ã¯ä½œå“ã®ä¸­ã§ä½•ã‚’æŒ‡ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ [/INST]  ONE PIECEã¨ã¯ã€æ±äº¬ãƒˆãƒªãƒƒãƒ—ã®å°¾ç”°æ „ä¸€éƒã•ã‚“ã«ã‚ˆã‚‹æ¼«ç”»ä½œå“ã®åç§°ã§ã™ã€‚\n\nã¾ãŸã€ã“ã®ä½œå“ã¯ã€ä¸»äººå…¬ã®ãƒ¢ãƒ³ã‚­ãƒ¼ãƒ»Dãƒ»ãƒ«ãƒ•ã‚£ã¨ãã®ä»²é–“ãŸã¡ãŒã€ä¸–ç•Œä¸€ã®è³é‡‘ç¨¼ãã‚’ç›®æŒ‡ã—ã¦å†’é™ºã‚’ç¹°ã‚Šåºƒã’ã‚‹ã¨ã„ã†ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚'

```python
result = qa("ã€ŒONE PIECEã€ã¨ã¯ä½œå“ã®ä¸­ã§ä½•ã‚’æŒ‡ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ")
print('å›ç­”:', result['result'])
print('='*10)
print('ã‚½ãƒ¼ã‚¹:', result['source_documents'])
```

<style>
  pre {
      white-space: pre-wrap;
  }
</style>

    [1m> Entering new RetrievalQA chain...[0m

    [1m> Finished chain.[0m
    å›ç­”:  ã€ŒONE PIECEã€ã¨ã¯ä½œå“ã®ä¸­ã§ã€ä»¥ä¸‹ã‚’æŒ‡ã—ã¾ã™ã€‚

    - æ¼«ç”»
    - ã‚¢ãƒ‹ãƒ¡
    - ã‚²ãƒ¼ãƒ 
    - æ˜ ç”»
    - ãƒ†ãƒ¬ãƒ“ãƒ‰ãƒ©ãƒ
    - èˆå°
    - å°èª¬
    - ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼
    - ä½œå“å…¨èˆ¬

    è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã€ŒONE PIECEã€ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚
    ==========
    ã‚½ãƒ¼ã‚¹: [Document(page_content='ONE PIECE\nã€ONE PIECEã€ï¼ˆãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹ï¼‰ã¯ã€å°¾ç”°æ „ä¸€éƒã«ã‚ˆã‚‹æ—¥æœ¬ã®å°‘å¹´æ¼«ç”»ä½œå“ã€‚ã€é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—ã€ï¼ˆé›†è‹±ç¤¾ï¼‰ã«ã¦1997å¹´34å·ã‹ã‚‰é€£è¼‰ä¸­ã€‚ç•¥ç§°ã¯ã€Œãƒ¯ãƒ³ãƒ”ã€[3]ã€‚\n|ONE PIECE|\n|ã‚¸ãƒ£ãƒ³ãƒ«||å°‘å¹´æ¼«ç”»ãƒ»æµ·è³Šãƒ»å†’é™º|\nãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ãƒ»ãƒãƒˆãƒ«\n|æ¼«ç”»|\n|ä½œè€…||å°¾ç”°æ „ä¸€éƒ|\n|å‡ºç‰ˆç¤¾||é›†è‹±ç¤¾|\n|\n|\n|æ²è¼‰èªŒ||é€±åˆŠå°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—|\n|ãƒ¬ãƒ¼ãƒ™ãƒ«||ã‚¸ãƒ£ãƒ³ãƒ—ãƒ»ã‚³ãƒŸãƒƒã‚¯ã‚¹|\n|ç™ºè¡¨å·||1997å¹´34å· -|\n|ç™ºè¡¨æœŸé–“||1997å¹´7æœˆ22æ—¥[1] -|', metadata={'source': 'textfile.txt'}), Document(page_content='- ^ "æ¼«ç”»å…¨å·»ãƒ‰ãƒƒãƒˆã‚³ãƒ  2012å¹´ å¹´é–“ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ™ã‚¹ãƒˆ1000ã‚’ç™ºè¡¨". PRTIMES. 2012å¹´12æœˆ5æ—¥. 2012å¹´12æœˆ7æ—¥é–²è¦§ã€‚\n- ^ "ã€ONE PIECEã€å…¨56å·»ã€ã‚³ãƒŸãƒƒã‚¯ã‚¹éƒ¨é–€200ä½ä»¥å†…ã«ç™»å ´". ã‚ªãƒªã‚³ãƒ³ãƒ‹ãƒ¥ãƒ¼ã‚¹. ã‚ªãƒªã‚³ãƒ³. 2009å¹´12æœˆ17æ—¥. 2011å¹´10æœˆ31æ—¥é–²è¦§ã€‚\n- ^ æ—¥çµŒã‚¨ãƒ³ã‚¿ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ!ã€2010å¹´7æœˆ4æ—¥ç™ºè¡Œã€79é \n- ^ "ã€ONE PIECEã€æœ€æ–°100å·»ãŒã‚³ãƒŸãƒƒã‚¯1ä½ æ—¢åˆŠ100å·»å…¨ã¦ãŒç´¯ç©å£²ä¸Š100ä¸‡éƒ¨çªç ´ã€ã‚ªãƒªã‚³ãƒ³ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘". ã‚ªãƒªã‚³ãƒ³ãƒ‹ãƒ¥ãƒ¼ã‚¹. ã‚ªãƒªã‚³ãƒ³. 2021å¹´9æœˆ10æ—¥. 2021å¹´9æœˆ12æ—¥é–²è¦§ã€‚', metadata={'source': 'textfile.txt'}), Document(page_content='- 9æœˆ1æ—¥ - ã€ONE PIECE FILM REDã€ã®ä¸»é¡Œæ­Œã§ã‚ã‚‹Adoã®ã€Œæ–°æ™‚ä»£ã€ãŒã€Apple Musicã®ä¸–ç•Œã§æœ€ã‚‚å†ç”Ÿã•ã‚Œã¦ã„ã‚‹æ¥½æ›²ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒãƒ£ãƒ¼ãƒˆã€Œãƒˆãƒƒãƒ—100ï¼šã‚°ãƒ­ãƒ¼ãƒãƒ«ã€ã§ç¬¬1ä½ã‚’ç²å¾—[26][27]ã€‚åŒãƒãƒ£ãƒ¼ãƒˆã§æ—¥æœ¬ã®æ¥½æ›²ãŒ1ä½ã«è¼ãã®ã¯å²ä¸Šåˆ[26][27]ã€‚\n- 9æœˆ1æ—¥ã€œ12æœˆ1æ—¥ - æ¼«ç”»ã‚¢ãƒ—ãƒªã€å°‘å¹´ã‚¸ãƒ£ãƒ³ãƒ—+ã€ã¨ç·åˆé›»å­æ›¸åº—ã€Œã‚¼ãƒ–ãƒ©ãƒƒã‚¯ã€ã«ã¦ã€æ¼«ç”»ã€ONE PIECEã€90å·»åˆ†ãŒ8æ®µéšã«åˆ†ã‘ã¦ç„¡æ–™å…¬é–‹ã•ã‚Œã‚‹[28]ã€‚\n- 2023å¹´ï¼ˆä»¤å’Œ5å¹´ï¼‰\nã‚ã‚‰ã™ã˜ ç·¨é›†', metadata={'source': 'textfile.txt'})]

## çµè«–

RAG ã«ã‚ˆã‚Šå›ç­”ã®è³ªãŒå…¨ä½“çš„ã«ã‹ãªã‚Šä¸ŠãŒã£ãŸã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚

ä½™è«‡ï¼šæœ€å¾Œã®è³ªå•ã«å¯¾ã™ã‚‹ GPT-4 ã® RAG ãªã—ã§ã®å›ç­”ã¯ä¸‹è¨˜ã§ã—ãŸã€‚æµçŸ³ã§ã™ã­:

`ã€ŒONE PIECEã€ã¯ã€æ—¥æœ¬ã®æ¼«ç”»å®¶å°¾ç”°æ „ä¸€éƒï¼ˆEiichiro Odaï¼‰ã«ã‚ˆã£ã¦ä½œã‚‰ã‚ŒãŸæ¼«ç”»ãŠã‚ˆã³ã‚¢ãƒ‹ãƒ¡ä½œå“ã§ã‚ã‚Šã€ãã®ä¸­ã§ã€ŒOne Pieceã€ã¨ã¯ã€ä¼èª¬çš„ãªæµ·è³Šã‚´ãƒ¼ãƒ«ãƒ»Dãƒ»ãƒ­ã‚¸ãƒ£ãƒ¼ãŒæ®‹ã—ãŸã¨ã•ã‚Œã‚‹ã€ä¸–ç•Œæœ€å¤§ã®è²¡å®ã‚’æŒ‡ã—ã¾ã™ã€‚ã“ã®è²¡å®ã¯ã€æœ€ã‚‚å±é™ºã§æœªçŸ¥ãªæµ·åŸŸã§ã‚ã‚‹ã€Œå‰å¤§ãªã‚‹èˆªè·¯ï¼ˆGrand Lineï¼‰ã€ã®æœ€å¾Œã«ã‚ã‚‹ã€Œãƒ©ãƒ•ãƒ†ãƒ«ã€ã¨ã„ã†å³¶ã«éš ã•ã‚Œã¦ã„ã‚‹ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚`

## å‚è€ƒ

ã“ã¡ã‚‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆã™ã‚‹ã«ã‚ãŸã‚Šã€ä¸‹è¨˜ã‚’å‚è€ƒã«ã•ã›ã¦ã„ãŸã ã„ã¦ãŠã‚Šã¾ã™ã€‚

- [alfredplpl/RetrievalQA.py](https://gist.github.com/alfredplpl/57a6338bce8a00de9c9d95bbf1a6d06d)
- [Langchain Docs](https://python.langchain.com/docs/get_started/introduction)
- [Wikipediaã€ŒONE_PIECEã€](https://ja.m.wikipedia.org/wiki/ONE_PIECE)
