{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2NYeLEU3pgom"
      },
      "source": [
        "# Elyza 7bを用いてRAGを試してみた。\n",
        "\n",
        "今回はLLMにはElyza 7B Instructを用い、Langchainを使ったRAG (Retrieval Augmented Generation) を試してみました。\n",
        "\n",
        "RAGを用いることで質問に対して関連性の高い文章を抽出し、より適切な答えを導き出せることを期待します。\n",
        "\n",
        "[Open in Colab](https://colab.research.google.com/drive/1Fe3LEtkTfLHpGM94gfznYoIhabm_QLY8?usp=sharing)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MSgbKpUnpcTj"
      },
      "source": [
        "## 必要なライブラリをインストール\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UvYuJwdF6vps"
      },
      "outputs": [],
      "source": [
        "# To solve for an error encountered: `NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968`\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# 必要なライブラリをインストール\n",
        "!pip install transformers langchain accelerate bitsandbytes pypdf tiktoken sentence_transformers faiss-gpu trafilatura --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dk1El-ootXbv"
      },
      "outputs": [],
      "source": [
        "# テキストが見やすいようにwrapしておく\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z-MXidABqFwl"
      },
      "source": [
        "## コードを実行\n",
        "\n",
        "必要なライブラリをロードします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "FYPTXrKI8bpc",
        "outputId": "6e4b7262-85dc-475e-cc0d-32089ee2f8a9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain import PromptTemplate"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rwDtQT1oqqDX"
      },
      "source": [
        "## データソースを準備\n",
        "\n",
        "今回はウィキペディア上にある「ONE PIECE」のページをデータソースとして、それに関連する質問をしていきたいと思います。\n",
        "\n",
        "* https://ja.m.wikipedia.org/wiki/ONE_PIECE\n",
        "\n",
        "今回はウェブページのテキストだけを抽出してくれる `trafilatura` というライブラリを用いましたが、Langchain内にもこれ用の `BSHTMLLoader` というのがあるようです。まだ試せていません。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DrKZo9g4jg8z"
      },
      "outputs": [],
      "source": [
        "# https://python.langchain.com/docs/modules/data_connection/document_loaders/html から引用。\n",
        "# from langchain.document_loaders import BSHTMLLoader\n",
        "\n",
        "# loader = BSHTMLLoader(\"example_data/fake-content.html\")\n",
        "# data = loader.load()\n",
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "OjHyDNg9iPYN",
        "outputId": "1f2cf610-dbca-4d72-8b2e-5897b9f28276"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONE PIECE\n",
            "『ONE PIECE』（ワンピース）は、尾田栄一郎による日本の少年漫画作品。『週刊少年ジャンプ』（集英社）にて1997年34号から連載中。略称は「ワンピ」[3]。\n",
            "|ONE PIECE|\n",
            "|ジャンル||少年漫画・海賊・冒険|\n",
            "ファンタジー・バトル\n",
            "|漫画|\n",
            "|作者||尾田栄一郎|\n",
            "|出版社||集英社|\n",
            "|\n",
            "|\n",
            "|掲載誌||週刊少年ジャンプ|\n",
            "|レーベル||ジャンプ・コミックス|\n",
            "|発表号||1997年34号 -|\n",
            "|発表期間||1997年7月22日[1] -|\n",
            "|巻数||既刊106巻（2023年7月4日）|\n",
            "|話数||既刊1090話（2023年8月21日[2]）|\n",
            "|テンプレート - ノート|\n",
            "|プロジェクト||漫画|\n",
            "|ポータル||漫画|\n",
            "概要 編集\n",
            "海賊王を夢見る少年モンキー・D・ルフィを主人公とする「ひとつなぎの大秘宝（ワンピース）」を巡る海洋冒険ロマン。\n",
            "夢への冒険・仲間たちとの友情といったテーマを前面に掲げ、バトルやギャグシーン、感動エピソードをメインとする少年漫画の王道を行く物語として人気を博している[4]。また、長年にわたりながら深く練り込まれた壮大な世界観・巧緻な設定のストーリーも特徴。\n",
            "2023年8月の時点で単行本は第106巻まで刊行されており、『週刊少年ジャンプ』歴代作品の中では『こちら葛飾区亀有公園前派出所』（1976年 - 2016年）に次ぐ長期連載となっている。国内累計発行部数は2022年時点で日本の漫画では最高となる4億1000万部を突破している[5]。また第67巻は初版発行部数405万部[6]の国内出版史上最高記録を樹立し、第57巻（2010年3月発売）以降の単行本は初版300万部以上発行を継続する[7]など出版の国内最高記録をいくつも保持している。\n",
            "2015年6月15日には \"Most Copies Published For The Same Comic Book Series By A Single Author（最も多く発行された単一作者によるコミックシリーズ）\" 名義でギネス世界記録に認定された[8][9]。実績は発行部数3億2,086万6,000部（2014年12月時点）[8]。なおこのギネス世界記録は2022年7月付で同作品によって更新され[10]、日本では同年8月に「日本国内累計発行部数4億1656万\n"
          ]
        }
      ],
      "source": [
        "from trafilatura import fetch_url, extract\n",
        "\n",
        "url = \"https://ja.m.wikipedia.org/wiki/ONE_PIECE\"\n",
        "filename = 'textfile.txt'\n",
        "\n",
        "document = fetch_url(url)\n",
        "text = extract(document)\n",
        "print(text[:1000])\n",
        "\n",
        "with open(filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m87JvJNtrhjW"
      },
      "source": [
        "抽出したテキストをテキストファイルに保存出来ました。このテキストファイルをLangchainのTextSplitterを使って小口のチャンクに切っていきます。\n",
        "\n",
        "こうして生成したチャンクからembeddingを生成し、質問のembeddingに一番近いトップｋのチャンクを抽出。そのテキストをプロンプト内に突っ込み、質問と同時にLLMに投げて回答を得る。といった流れとなります。\n",
        "\n",
        "私は少なくともそういう理解です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        },
        "id": "O8XrzuC48-fh",
        "outputId": "fb2fd3fd-bf74-4846-f290-ff4376b730ce"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 361, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 388, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 333, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 301, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 336, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 540, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 464, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 366, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 331, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 327, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 409, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 442, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 389, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 357, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 425, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 370, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 399, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 345, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 531, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 390, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 504, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 387, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 567, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 478, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 466, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 383, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 374, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 347, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 492, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 435, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 350, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 416, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 331, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 453, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 319, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 763, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 417, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 372, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 450, which is longer than the specified 300\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 358, which is longer than the specified 300\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "493\n"
          ]
        }
      ],
      "source": [
        "loader = TextLoader(filename, encoding='utf-8')\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)\n",
        "print(len(texts))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Vm3O_ssRbW"
      },
      "source": [
        "どんな構造をしているのか知るために、何個か見てみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "tvao6Dt7eXCp",
        "outputId": "1b9388bd-6c1e-4e03-9d03-5ee25bd4e9c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[Document(page_content='アラバスタ編 編集\\n- 【12巻 - 23巻】\\n-\\n- 偉大なる航路突入（12巻）\\n- 麦わらの一味はついに「偉大なる航路」に突入する。リヴァース・マウンテンを降りた場所にある「双子岬」で、仲間の帰還を待ち続けるクジラ・ラブーンと出会う。ルフィはラブーンと、「偉大なる航路」一周後に再戦する約束を交わす。\\n- ウイスキーピーク編（12巻 - 13巻）', metadata={'source': 'textfile.txt'}),\n",
              " Document(page_content='- ルフィ達は、最初の島「サボテン島」の町「ウイスキーピーク」で大歓迎を受ける。だがその町は、秘密犯罪会社「バロックワークス」（B・W）の社員である賞金稼ぎ達の巣であった。そこで一味は、B・Wエージェントの一人の正体が、「偉大なる航路」にある大国「アラバスタ王国」の王女ネフェルタリ・ビビであると知る。B・Wに潜入していた彼女から、ルフィ達はB・Wによるアラバスタ王国乗っ取り計画を知る。ビビを一行に加えた麦わらの一味は、B・Wからの追手を振り切りつつ、計画を阻止すべく一路アラバスタを目指す。', metadata={'source': 'textfile.txt'}),\n",
              " Document(page_content='- リトルガーデン編（13巻 - 15巻）\\n- ウイスキーピークを出港したルフィ達は、ジャングルの中で恐竜達が生きる太古の島「リトルガーデン」に上陸する。ルフィ達はその島で、巨人族の二人の戦士・ドリーとブロギーに出会う。彼らは「誇り」を守るため、100年間も決闘を続けてきたという。だがその決闘が、B・Wからの追手による卑劣な策略で邪魔される。ルフィ達はB・Wエージェントにして姑息な美術家・Mr.3らを破り、巨人族の誇りを守る。\\n- ドラム島編（15巻 - 17巻）', metadata={'source': 'textfile.txt'})]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[30:33]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vqbGRqeGsUPL"
      },
      "source": [
        "## Embeddingの生成とFAISSを使ったベクトルDBの用意\n",
        "\n",
        "小口のチャンクに切ったテキストを、Embeddingモデルを使ってembeddingに変換していきます。テキストの類似性をもとに検索をできるようにするためです。\n",
        "\n",
        "Embeddingの生成には `intfloat/multilingual-e5-large` を使います。\n",
        "\n",
        "ベクトルDBには `FAISS` のライブラリを用います。（今回はGPUのある環境で走らせてみているため、 faiss-gpu をロードしています。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UgTRglxF9DEL",
        "outputId": "5b866ee5-90f1-4f07-dc32-a96b59655bd2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "# 一番類似するチャンクをいくつロードするかを変数kに設定出来ます。\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R3NlvklczFgm"
      },
      "source": [
        "今回の環境ではembeddingを用意するのに25秒ほどかかりました。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xiJfjjx1yBRt"
      },
      "source": [
        "## モデルの用意\n",
        "\n",
        "今回はElyza-7b-instructを用います。\n",
        "\n",
        "今回はColabのT4でも問題なく実行できるよう、BitsandBytesで4bitに量子化したものをロードします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c5c7e3a624674affb967345b53ece33f",
            "3a508c9563a54e0aadf1f75633dc7894",
            "19e70cc6b19543c9bf89deb752d54049",
            "fb72e3e427e7439093917259e8962a0d",
            "1fb2bef2243143d6afebebbed1f6449d",
            "385545493e104928889e7267356d077e",
            "5b8bc3e88d8c492d8b690b231c0987a3",
            "95d32b26009c406286eaa05ade520aa8",
            "6f270f68d4fa4ebca2d5540c58065e0f",
            "6bb089fc46fe413dbb3d28b450bae468",
            "24d717273ff34614b8cc79dbe6dded45"
          ]
        },
        "id": "23g2UJetyHC5",
        "outputId": "cd3e67bb-cf78-4ff1-96fb-c901a5cc4521"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5c7e3a624674affb967345b53ece33f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ").eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gvdx0XjhzTdQ"
      },
      "source": [
        "次に、Elyza-7b-instruct用のプロンプトテンプレートを用意します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "h9aWJ-scyKBL",
        "outputId": "5788aa00-a6cc-471a-b69e-cf3d5b0c8f1a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"参考情報を元に、ユーザーからの質問にできるだけ正確に答えてください。\"\n",
        "text = \"{context}\\nユーザからの質問は次のとおりです。{question}\"\n",
        "template = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
        "    bos_token=tokenizer.bos_token,\n",
        "    b_inst=B_INST,\n",
        "    system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
        "    prompt=text,\n",
        "    e_inst=E_INST,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pFsez5nts9nR"
      },
      "source": [
        "## LLMとChainの指定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dwCNAHhycztp",
        "outputId": "1889672f-d76a-4895-e793-dae72f211926"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "PROMPT = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"question\",\"context\"],\n",
        "    template_format=\"f-string\"\n",
        ")\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=HuggingFacePipeline(\n",
        "        pipeline=pipe,\n",
        "        # model_kwargs=dict(temperature=0.1, do_sample=True, repetition_penalty=1.1)\n",
        "    ),\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7QTttq52tRwb"
      },
      "source": [
        "## お試し\n",
        "\n",
        "要約質問ができる状態が整いました。\n",
        "最初にRAGを使わずにLLMに質問をし、その後にRAGを使って生成してみて差分を比較してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "XrgIqlNnzm9K",
        "outputId": "9a5b29a1-cdf4-4ff1-aa71-936920dcdbda"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[INST] <<SYS>>\\n参考情報を元に、ユーザーからの質問にできるだけ正確に答えてください。\\n<</SYS>>\\n\\n\\nユーザからの質問は次のとおりです。ニコ・ロビンの職業は何ですか？ [/INST]  ニコ・ロビンの職業は、強盗団の一員です。'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = template.format(context='', question='ニコ・ロビンの職業は何ですか？')\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Mi0Wcc6ad4FN",
        "outputId": "90134db2-c2d1-4492-909b-5c361f92fdae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "回答:  ニコ・ロビンの職業は「考古学者」です。\n",
            "==========\n",
            "ソース: [Document(page_content='空島編 編集\\n- 【24巻 - 32巻】\\n-\\n- ジャヤ編（24巻 - 25巻）\\n- アラバスタを後にしたルフィ達は、B・W社副社長であった考古学者ニコ・ロビンを仲間に加える。次の島に向かう航海中、突如空から巨大なガレオン船が落下し、「記録指針（ログポース）」の指す進路が上向きに変更される。それは伝説とされる空に浮かぶ島「空島」への指針を意味していた。', metadata={'source': 'textfile.txt'}), Document(page_content='- 「THE 8TH LOG \"SKYPIEA\"」2008年4月発行、ISBN 978-4-08-111027-8\\n- 「THE 9TH LOG \"GOD\"」2008年5月発行、ISBN 978-4-08-111028-5\\n- 「THE 10TH LOG \"BELL\"」2008年6月発行、ISBN 978-4-08-111029-2\\n- 「THE 11TH LOG \"WATER SEVEN\"」2009年4月発行、ISBN 978-4-08-111009-4\\n- 「THE 12TH LOG \"ROCKET MAN\"」2009年5月発行、ISBN 978-4-08-111010-0\\n- 「THE 13TH LOG \"NICO ROBIN\"」2009年7月発行、ISBN 978-4-08-111011-7\\n- 「THE 14TH LOG \"FRANKY\"」2009年8月発行、ISBN 978-4-08-111012-4', metadata={'source': 'textfile.txt'}), Document(page_content='- 声 - 大谷育江\\n- 麦わらの一味船医。「ヒトヒトの実」を食べ人の能力を持った人間トナカイ。万能薬（何でも治せる医者）を目指している。\\n- ニコ・ロビン\\n- 声 - 山口由里子\\n- 麦わらの一味考古学者。「ハナハナの実」の能力者。歴史上の「空白の100年」の謎を解き明かすため旅をしている。\\n- フランキー\\n- 声 - 矢尾一樹', metadata={'source': 'textfile.txt'})]\n"
          ]
        }
      ],
      "source": [
        "result = qa(\"ニコ・ロビンの職業は何ですか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TygOY0yO3SBf",
        "outputId": "7b6bfcb2-deb8-411f-b2cf-b728681931bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[INST] <<SYS>>\\n参考情報を元に、ユーザーからの質問にできるだけ正確に答えてください。\\n<</SYS>>\\n\\n\\nユーザからの質問は次のとおりです。エネルは何者ですか？ [/INST]  エネルは、エネルギーのことです。'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = template.format(context='', question='エネルは何者ですか？')\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "MIgAIwW_d5bP",
        "outputId": "ec7c8c34-8afc-4d9b-d02a-51cf165e8af6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "回答:  エネルは、ONE PIECEの登場人物である神の国「スカイピア」の神の一人です。神の国「スカイピア」は、かつて地上に存在した伝説の黄金郷である「神の島（アッパーヤード）」が、から支配しています。エネルは、神の軍団を率いています。\n",
            "==========\n",
            "ソース: [Document(page_content='- ルフィ達は上空1万メートルにある空島に辿り着く。そこには今まで全く見たことがない未知の文化が広がっていた。ルフィ達は、神の国「スカイピア」で上陸した「神の島（アッパーヤード）」が、かつて地上に存在した伝説の黄金郷であることをつきとめる。しかし、そこは神の軍団を率いる〝神・エネル〟が支配する土地であり、空の民と島の先住民シャンディアが400年に渡り争い続けている土地であった。黄金捜しに乗り出したルフィ達は、神の軍団とシャンディアとの過酷なサバイバルに巻き込まれる。エネルの圧倒的な力に多くの戦士たちが倒れていき、エネルによって空島は消滅の危機に陥る。だが、唯一エネルに対抗できるルフィによって空島の危機は防がれ、400年に渡る空の民とシャンディアの争いに終止符が打たれた。', metadata={'source': 'textfile.txt'}), Document(page_content='- 一方、サニー号は巨大なロボットに捕まってエッグヘッドに連行される。ゾロたちはベガパンクの分身の「悪(リリス)」と「正(シャカ)」により、研究所に通されることになる。研究所に着くと、一味はジンベエそっくりの新型パシフィスタ「セラフィム」の襲撃を受け戦闘データが収集されるが、セラフィムを破壊される前に正(シャカ)が戦闘を中止させる。正(シャカ)は、この島が「過去」であり、この島のような高度な文明を持った王国が900年前に実在していたと語る。', metadata={'source': 'textfile.txt'}), Document(page_content='ノベライズ作品 編集\\n集英社の新書レーベル「JUMP j BOOKS」より発売されている、アニメオリジナルストーリーや劇場版のノベライズ作品。一部は児童文学レーベル「集英社みらい文庫」でも刊行されている。\\nその他の小説作品 編集\\n- ONE PIECE novel A（エース）\\n- エースを主人公とし、スペード海賊団時代の冒険を描く。ムック『ONE PIECE magazine』Vol.1からVol.3まで連載され[162][163]、後に第1巻として書籍化された。著者はひなたしょう。', metadata={'source': 'textfile.txt'})]\n"
          ]
        }
      ],
      "source": [
        "result = qa(\"エネルは何者ですか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "y0Sf2Hbm39Rd",
        "outputId": "ff573cc0-061e-4311-996c-0c58c78fad91"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[INST] <<SYS>>\\n参考情報を元に、ユーザーからの質問にできるだけ正確に答えてください。\\n<</SYS>>\\n\\n\\nユーザからの質問は次のとおりです。チョッパーの特殊能力は何ですか？ [/INST]  チョッパーの特殊能力について回答いたします。\\n\\nチョッパーは、相手の攻撃を受けてもその攻撃を相手に返してくることのできる「リフレクター」という能力を持っています。これにより、チョッパーは攻撃を受けることが多く、守備力が低いという傾向にあります。'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = template.format(context='', question='チョッパーの特殊能力は何ですか？')\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "KclpqfLUwkwT",
        "outputId": "fe67fa92-642c-486a-dab4-3a7134e55610"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "回答:  チョッパーの特殊能力は、人の能力を持つトナカイであるため、その能力は「人のように歩く」ことです。\n",
            "\n",
            "また、チョッパーは元々人間ではなく、悪魔の実の能力で人のように歩く能力を得たトナカイです。\n",
            "==========\n",
            "ソース: [Document(page_content='- チョッパーマン\\n- パラレルワールドを舞台に、チョッパーを主人公にしたスピンオフ漫画。作画は武井宏文。『最強ジャンプ』2012年1月号から2014年2月号まで連載された。\\n- ワンピースパーティー\\n- SD化したキャラたちが繰り広げる、スピンオフギャグ漫画。作画は安藤英。『最強ジャンプ』2015年1月号より連載中。\\n- CHIN PIECE[38]', metadata={'source': 'textfile.txt'}), Document(page_content='- 11月11日 - 単行本国内累計発行部数が2億冊を突破（第60巻）[15]。\\n- 2011年（平成23年）\\n- 4月 - 『週刊少年ジャンプ 2011年4月4日号 No.17』に島袋光年の『トリコ』とのクロスオーバー作品『実食! 悪魔の実!!』が掲載される。\\n- 12月3日 - 『最強ジャンプ 2012年1月号（2011年12月3日発売号）』より、スピンオフ漫画『チョッパーマン』が連載開始。\\n- 2012年（平成24年）', metadata={'source': 'textfile.txt'}), Document(page_content='- リトルガーデン出港後、ナミが急病に倒れてしまう。急遽進路を変更し、雪の島「ドラム島」に立ち寄った麦わらの一味は、悪魔の実を食べ人の能力を持ったトナカイ、トニートニー・チョッパーと出会う。ルフィはチョッパーを仲間に誘うが、彼には悲しき過去があった。そこへ、かつて島で悪政を敷いた元ドラム王国国王ワポルが帰還する。ルフィ達はチョッパーと共闘してワポルを撃退し、船医チョッパーを仲間に迎える。\\n- アラバスタ編（17巻 - 23巻）', metadata={'source': 'textfile.txt'})]\n"
          ]
        }
      ],
      "source": [
        "result = qa(\"チョッパーの特殊能力は何ですか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "NftahnE44MVS",
        "outputId": "b38686b1-8ac5-4684-c3de-d4180eebc9ea"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[INST] <<SYS>>\\n参考情報を元に、ユーザーからの質問にできるだけ正確に答えてください。\\n<</SYS>>\\n\\n\\nユーザからの質問は次のとおりです。「ONE PIECE」とは作品の中で何を指していますか？ [/INST]  ONE PIECEとは、東京トリップの尾田栄一郎さんによる漫画作品の名称です。\\n\\nまた、この作品は、主人公のモンキー・D・ルフィとその仲間たちが、世界一の賞金稼ぎを目指して冒険を繰り広げるというストーリーで構成されています。'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = template.format(context='', question=\"「ONE PIECE」とは作品の中で何を指していますか？\")\n",
        "inputs = tokenizer(inputs, return_tensors='pt').to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "output = tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=True)\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "priK5mYmt3q_",
        "outputId": "d9e3525a-fa4d-4bbf-df22-4838a0dbe22b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "回答:  「ONE PIECE」とは作品の中で、以下を指します。\n",
            "\n",
            "- 漫画\n",
            "- アニメ\n",
            "- ゲーム\n",
            "- 映画\n",
            "- テレビドラマ\n",
            "- 舞台\n",
            "- 小説\n",
            "- キャラクター\n",
            "- 作品全般\n",
            "\n",
            "詳細については、「ONE PIECE」を参考にしてください。\n",
            "==========\n",
            "ソース: [Document(page_content='ONE PIECE\\n『ONE PIECE』（ワンピース）は、尾田栄一郎による日本の少年漫画作品。『週刊少年ジャンプ』（集英社）にて1997年34号から連載中。略称は「ワンピ」[3]。\\n|ONE PIECE|\\n|ジャンル||少年漫画・海賊・冒険|\\nファンタジー・バトル\\n|漫画|\\n|作者||尾田栄一郎|\\n|出版社||集英社|\\n|\\n|\\n|掲載誌||週刊少年ジャンプ|\\n|レーベル||ジャンプ・コミックス|\\n|発表号||1997年34号 -|\\n|発表期間||1997年7月22日[1] -|', metadata={'source': 'textfile.txt'}), Document(page_content='- ^ \"漫画全巻ドットコム 2012年 年間ランキングベスト1000を発表\". PRTIMES. 2012年12月5日. 2012年12月7日閲覧。\\n- ^ \"『ONE PIECE』全56巻、コミックス部門200位以内に登場\". オリコンニュース. オリコン. 2009年12月17日. 2011年10月31日閲覧。\\n- ^ 日経エンタテイメント!、2010年7月4日発行、79頁\\n- ^ \"『ONE PIECE』最新100巻がコミック1位 既刊100巻全てが累積売上100万部突破【オリコンランキング】\". オリコンニュース. オリコン. 2021年9月10日. 2021年9月12日閲覧。', metadata={'source': 'textfile.txt'}), Document(page_content='- 9月1日 - 『ONE PIECE FILM RED』の主題歌であるAdoの「新時代」が、Apple Musicの世界で最も再生されている楽曲のデイリーチャート「トップ100：グローバル」で第1位を獲得[26][27]。同チャートで日本の楽曲が1位に輝くのは史上初[26][27]。\\n- 9月1日〜12月1日 - 漫画アプリ『少年ジャンプ+』と総合電子書店「ゼブラック」にて、漫画『ONE PIECE』90巻分が8段階に分けて無料公開される[28]。\\n- 2023年（令和5年）\\nあらすじ 編集', metadata={'source': 'textfile.txt'})]\n"
          ]
        }
      ],
      "source": [
        "result = qa(\"「ONE PIECE」とは作品の中で何を指していますか？\")\n",
        "print('回答:', result['result'])\n",
        "print('='*10)\n",
        "print('ソース:', result['source_documents'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TMzE0qqA4oPT"
      },
      "source": [
        "## 結論\n",
        "\n",
        "RAGにより回答の質が全体的にかなり上がったことが確認できました。\n",
        "\n",
        "余談：最後の質問に対するGPT-4のRAGなしでの回答は下記でした。流石ですね:\n",
        "\n",
        "`\n",
        "「ONE PIECE」は、日本の漫画家尾田栄一郎（Eiichiro Oda）によって作られた漫画およびアニメ作品であり、その中で「One Piece」とは、伝説的な海賊ゴール・D・ロジャーが残したとされる、世界最大の財宝を指します。この財宝は、最も危険で未知な海域である「偉大なる航路（Grand Line）」の最後にある「ラフテル」という島に隠されているとされています。\n",
        "`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "peRYCXDh5s77"
      },
      "source": [
        "## 参考\n",
        "\n",
        "こちらのノートブックを作成するにあたり、下記を参考にさせていただいております。\n",
        "\n",
        "* [alfredplpl/RetrievalQA.py](https://gist.github.com/alfredplpl/57a6338bce8a00de9c9d95bbf1a6d06d)\n",
        "* [Langchain Docs](https://python.langchain.com/docs/get_started/introduction)\n",
        "* [Wikipedia「ONE_PIECE」](https://ja.m.wikipedia.org/wiki/ONE_PIECE)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19e70cc6b19543c9bf89deb752d54049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95d32b26009c406286eaa05ade520aa8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f270f68d4fa4ebca2d5540c58065e0f",
            "value": 2
          }
        },
        "1fb2bef2243143d6afebebbed1f6449d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24d717273ff34614b8cc79dbe6dded45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "385545493e104928889e7267356d077e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a508c9563a54e0aadf1f75633dc7894": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_385545493e104928889e7267356d077e",
            "placeholder": "​",
            "style": "IPY_MODEL_5b8bc3e88d8c492d8b690b231c0987a3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5b8bc3e88d8c492d8b690b231c0987a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bb089fc46fe413dbb3d28b450bae468": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f270f68d4fa4ebca2d5540c58065e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95d32b26009c406286eaa05ade520aa8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5c7e3a624674affb967345b53ece33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a508c9563a54e0aadf1f75633dc7894",
              "IPY_MODEL_19e70cc6b19543c9bf89deb752d54049",
              "IPY_MODEL_fb72e3e427e7439093917259e8962a0d"
            ],
            "layout": "IPY_MODEL_1fb2bef2243143d6afebebbed1f6449d"
          }
        },
        "fb72e3e427e7439093917259e8962a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bb089fc46fe413dbb3d28b450bae468",
            "placeholder": "​",
            "style": "IPY_MODEL_24d717273ff34614b8cc79dbe6dded45",
            "value": " 2/2 [01:11&lt;00:00, 30.00s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
